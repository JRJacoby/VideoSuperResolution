{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = 10000\n",
    "sequence_length = 5\n",
    "train_data = np.empty((num_training_samples, sequence_length, 2)) # Hard-coded for 2 features\n",
    "\n",
    "for i in range(num_training_samples):\n",
    "    sample = np.array(random.randint(1000000, 1100000), ndmin=3) # [batch, timestep, feature]\n",
    "    features = np.random.rand(sequence_length)\n",
    "    features = np.reshape(features, (1, -1, 1))\n",
    "\n",
    "    for j in range(sequence_length - 1):\n",
    "        next_value = np.array([sample[:, -1, :] * 1.10 * features[:, j, :]])\n",
    "        sample = np.append(sample, next_value, axis=1)\n",
    "    \n",
    "    sample = np.append(sample, features, axis=2)\n",
    "    train_data[i, :, :] = sample\n",
    "\n",
    "mins_train = train_data.min(axis=0)\n",
    "train_data = (train_data - mins_train)\n",
    "maxes_train = train_data.max(axis=0)\n",
    "train_data = train_data / maxes_train\n",
    "train_data = torch.tensor(train_data, requires_grad = True, dtype=torch.float32)\n",
    "train_x = train_data[:, :, 1] # features\n",
    "train_y = train_data[:, :, 0] # sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_testing_samples = 10000\n",
    "sequence_length = 5\n",
    "test_data = np.empty((num_testing_samples, sequence_length, 2)) # Hard-coded for 2 features\n",
    "\n",
    "for i in range(num_testing_samples):\n",
    "    sample = np.array(random.randint(1000000, 1100000), ndmin=3) # [batch, timestep, feature]\n",
    "    features = np.random.rand(sequence_length)\n",
    "    features = np.reshape(features, (1, -1, 1))\n",
    "\n",
    "    for j in range(sequence_length - 1):\n",
    "        next_value = np.array([sample[:, -1, :] * 1.10 * features[:, j, :]])\n",
    "        sample = np.append(sample, next_value, axis=1)\n",
    "    \n",
    "    sample = np.append(sample, features, axis=2)\n",
    "    test_data[i, :, :] = sample\n",
    "\n",
    "mins_test = test_data.min(axis=0)\n",
    "test_data = (test_data - mins_test)\n",
    "maxes_test = test_data.max(axis=0)\n",
    "test_data = test_data / maxes_test\n",
    "test_data = torch.tensor(test_data, requires_grad = True, dtype=torch.float32)\n",
    "test_x = test_data[:, :, 1] # features\n",
    "test_y = test_data[:, :, 0] # sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRecurrent(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden_neurons):\n",
    "        super(CustomRecurrent, self).__init__()\n",
    "        \n",
    "        self.sequence_length = -1\n",
    "        self.batch_size = -1\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        \n",
    "        self.input_layer = nn.Linear(2, 10)\n",
    "        self.hidden_layer = nn.Linear(20, self.num_hidden_neurons)\n",
    "        self.output_layer = nn.Linear(self.num_hidden_neurons, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert(self.sequence_length > 0 and self.batch_size > 0)\n",
    "        y = self.input_layer(torch.cat([x[:, 0:1], torch.zeros((self.batch_size, 1))], dim=1))\n",
    "        y = torch.tanh(y)\n",
    "        y = self.hidden_layer(torch.cat([y, torch.zeros(self.batch_size, self.num_hidden_neurons)], dim=1))\n",
    "        y = torch.tanh(y)\n",
    "        hidden = y\n",
    "        output = self.output_layer(y)\n",
    "        for i in list(range(self.sequence_length - 1)):\n",
    "            y = self.input_layer(torch.cat([x[:, i+1:i+2], output[:, i:i+1]], dim=1))\n",
    "            y = torch.tanh(y)\n",
    "            y = self.hidden_layer(torch.cat([y, hidden], dim=1))\n",
    "            y = torch.tanh(y)\n",
    "            hidden = y\n",
    "            y = self.output_layer(y)\n",
    "            prev = y\n",
    "            output = torch.cat([output, y], dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def set_sequence_length(self, sequence_length):\n",
    "        assert(isinstance(sequence_length, int) or isinstance(sequence_length, float))\n",
    "        assert(sequence_length > 1)\n",
    "        if (isinstance(sequence_length, float)):\n",
    "            assert(sequence_length.is_integer())\n",
    "            \n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def set_batch_size(self, batch_size):\n",
    "        assert(isinstance(batch_size, int) or isinstance(batch_size, float))\n",
    "        assert(batch_size > 1)\n",
    "        if (isinstance(batch_size, float)):\n",
    "            assert(batch_size.is_integer())\n",
    "            \n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard summary of the training session. Variable A keeps track of session ID. Set in above cell before running training cell.\n",
    "writer = SummaryWriter('runs//run' + str(a))\n",
    "a += 1\n",
    "\n",
    "# Create a recurrent network. Constructor argument specifies number of hidden neurons. \n",
    "network = CustomRecurrent(10)\n",
    "network.set_batch_size(16)\n",
    "network.set_sequence_length(5)\n",
    "\n",
    "# Mean Squared Error Loss and an Rprop optimizer. Rprop seems to work best by far for recurrent networks.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Rprop(network.parameters(), etas=(0.3, 1.1), step_sizes=(1e-6, 1))\n",
    "\n",
    "# The first dimension of the tensor containing the input data should be the sample dimension. Also create lists for keeping track of loss throughout training. \n",
    "# The loop runs through all integers 0-num_training_samples which are multiples of batch_size. The (last num_training_samples % batch_size) samples are discarded.\n",
    "num_training_samples = train_data.size()[0]\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for i in list(range(0, num_training_samples, network.batch_size))[:-1]:\n",
    "    batch_x = train_x[i:i + network.batch_size, :]\n",
    "    batch_y = train_y[i:i + network.batch_size, :]\n",
    "    batch_x_val = test_x[i:i + network.batch_size, :]\n",
    "    batch_y_val = test_y[i:i + network.batch_size, :]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    network_output = network(batch_x)\n",
    "    loss = criterion(network_output[:, 1:], batch_y[:, 1:])\n",
    "    writer.add_scalar('training_loss', loss / (network.batch_size * network.sequence_length), i) # Keeps track of average loss per element of output matrix\n",
    "    training_losses.append(loss) # Keeps tracks of total loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate and record validation set loss\n",
    "    validation_loss = criterion(network(batch_x_val)[:, 1:], batch_y_val[:, 1:])\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    \n",
    "    # Keep track of the sum of gradients at each level of the network to help diagnose exploding/vanishing gradients.\n",
    "    named_params = list(network.named_parameters())\n",
    "    for j, parameter in enumerate(network.parameters()):\n",
    "        if i > 0:\n",
    "            writer.add_scalar('layer ' + str(named_params[j][0]) + ' gradients', torch.sum(parameter.grad)**2, i)\n",
    "    \n",
    "    \n",
    "# TensorBoard visual graph summary of the whole network.\n",
    "network.set_batch_size(num_training_samples)\n",
    "writer.add_graph(network, torch.Tensor(train_x))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([5.0], requires_grad = True)\n",
    "optimizer = optim.Rprop([tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor * torch.tensor([5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(network.parameters())[0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()\n",
    "print(torch.mean(torch.tensor(training_losses[-100:])))\n",
    "print(torch.mean(torch.tensor(validation_losses[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_val.detach().numpy() * maxes_test[:, 0].reshape((1, -1)) + mins_test[:, 0].reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x_val.detach().numpy() * maxes_train[:, 1].reshape((1, -1)) + mins_train[:, 1].reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.set_batch_size(16)\n",
    "output = network(batch_x_val)\n",
    "output.detach().numpy() * maxes_train[:, 0].reshape((1, -1)) + mins_train[:, 0].reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes_train[:, 0].reshape((1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): grad_fn = grad_fn.next_functions[0][0]; print(grad_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
